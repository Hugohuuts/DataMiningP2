{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []    \n",
    "# Hardcoded pad naar de hoofdmap\n",
    "base_path = r\"raw_files/negative_polarity\"   \n",
    "\n",
    "# Loop door de klassen (deceptive en truthful)\n",
    "for class_folder in os.listdir(base_path):\n",
    "    class_path = os.path.join(base_path, class_folder)\n",
    "        \n",
    "    if os.path.isdir(class_path):\n",
    "        subfolders = os.listdir(class_path)\n",
    "        # pak de eerste 4 folder als train set\n",
    "        for subfolder in subfolders[:-1]:  \n",
    "            subfolder_path = os.path.join(class_path, subfolder)\n",
    "            for file_name in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "\n",
    "                    #lowercase alle woorden\n",
    "                    content = file.read().lower()     \n",
    "                    # Verwijder engelse stopwoorden\n",
    "                    content = ' '.join([word for word in content.split() if word not in ENGLISH_STOP_WORDS])\n",
    "                        \n",
    "                    train_data.append((content, class_folder))\n",
    "            \n",
    "        # Pak de laatste folder als testset\n",
    "        test_subfolder = subfolders[-1]\n",
    "        test_subfolder_path = os.path.join(class_path, test_subfolder)\n",
    "        for file_name in os.listdir(test_subfolder_path):\n",
    "            file_path = os.path.join(test_subfolder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                #lowercase alle woorden\n",
    "                content = file.read().lower() \n",
    "                    \n",
    "                # Verwijder engelsestopwoorden\n",
    "                content = ' '.join([word for word in content.split() if word not in ENGLISH_STOP_WORDS])\n",
    "\n",
    "                test_data.append((content, class_folder))\n",
    "    \n",
    "    # Zet de gegevens om in een DataFrame\n",
    "    train_df = pd.DataFrame(train_data, columns=['text', 'label'])\n",
    "    test_df = pd.DataFrame(test_data, columns=['text', 'label'])\n",
    "\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text                 label\n",
      "0  stayed schicago hilton 4 days 3 nights confere...  deceptive_from_MTurk\n",
      "1  hotel located 1/2 mile train station quite hik...  deceptive_from_MTurk\n",
      "2  reservation hilton chicago believing going sta...  deceptive_from_MTurk\n",
      "3  people think hilton, think luxury. know did. w...  deceptive_from_MTurk\n",
      "4  husband recently stayed stayed hilton chicago ...  deceptive_from_MTurk\n",
      "                                                text                 label\n",
      "0  recently stayed hotel allegro chicago wife. bu...  deceptive_from_MTurk\n",
      "1  recently stayed hotel allegro chicago business...  deceptive_from_MTurk\n",
      "2  recently visited chicago. stayed hotel allegro...  deceptive_from_MTurk\n",
      "3  visited hotel allegro chicago vacation daughte...  deceptive_from_MTurk\n",
      "4  unimpressed quality hotel. overall look place ...  deceptive_from_MTurk\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 640 entries, 0 to 639\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    640 non-null    object\n",
      " 1   label   640 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 10.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160 entries, 0 to 159\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    160 non-null    object\n",
      " 1   label   160 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.6+ KB\n",
      "None\n",
      "                                                     text  \\\n",
      "count                                                 640   \n",
      "unique                                                636   \n",
      "top     disappointed stay chicago monoco. stayed times...   \n",
      "freq                                                    2   \n",
      "\n",
      "                       label  \n",
      "count                    640  \n",
      "unique                     2  \n",
      "top     deceptive_from_MTurk  \n",
      "freq                     320  \n",
      "                                                     text  \\\n",
      "count                                                 160   \n",
      "unique                                                160   \n",
      "top     recently stayed hotel allegro chicago wife. bu...   \n",
      "freq                                                    1   \n",
      "\n",
      "                       label  \n",
      "count                    160  \n",
      "unique                     2  \n",
      "top     deceptive_from_MTurk  \n",
      "freq                      80  \n",
      "label\n",
      "deceptive_from_MTurk    320\n",
      "truthful_from_Web       320\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "deceptive_from_MTurk    80\n",
      "truthful_from_Web       80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df.info())\n",
    "print(test_df.info())\n",
    "print(train_df.describe())\n",
    "print(test_df.describe())\n",
    "print(train_df['label'].value_counts())\n",
    "print(test_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train en test variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial_Naive_Bayes ({ngram_type}): \n",
      "Accuracy: 0.89375\n",
      "Precision: 0.8938115330520393\n",
      "Recall: 0.89375\n",
      "F1 Score: 0.8937458494472441\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__alpha': 0.5, 'vectorizer__min_df': 0.005} unigrams\n",
    "#{'classifier__alpha': 2, 'vectorizer__min_df': 0.005} bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), min_df = 0.005)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])    \n",
    "    \n",
    "#     # Labels voor training en testen\n",
    "# y_train = train_data['label']\n",
    "# y_test = test_data['label']\n",
    "    \n",
    "    # Initialiseer het Multinomial Naive Bayes model\n",
    "model_nb1 = MultinomialNB(alpha=0.5)   \n",
    "\n",
    "    # Train het model\n",
    "model_nb1.fit(X_train, y_train)\n",
    "    \n",
    "    # Voorspel de labels voor de testset\n",
    "y_pred1 = model_nb1.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    "precision = precision_score(y_test, y_pred1, average='weighted')\n",
    "recall = recall_score(y_test, y_pred1, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred1, average='weighted')\n",
    "\n",
    "print(\"Multinomial_Naive_Bayes ({ngram_type}): \")\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial_Naive_Bayes ({ngram_type}): \n",
      "Accuracy: 0.90625\n",
      "Precision: 0.9068220935690816\n",
      "Recall: 0.90625\n",
      "F1 Score: 0.906217029424407\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__alpha': 0.5, 'vectorizer__min_df': 0.005} unigrams\n",
    "#{'classifier__alpha': 2, 'vectorizer__min_df': 0.005} bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), min_df = 0.005)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])    \n",
    "    \n",
    "#     # Labels voor training en testen\n",
    "# y_train = train_data['label']\n",
    "# y_test = test_data['label']\n",
    "    \n",
    "    # Initialiseer het Multinomial Naive Bayes model\n",
    "model_nb2 = MultinomialNB(alpha=2)   \n",
    "\n",
    "    # Train het model\n",
    "model_nb2.fit(X_train, y_train)\n",
    "    \n",
    "    # Voorspel de labels voor de testset\n",
    "y_pred2 = model_nb2.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred2)\n",
    "precision = precision_score(y_test, y_pred2, average='weighted')\n",
    "recall = recall_score(y_test, y_pred2, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred2, average='weighted')\n",
    "\n",
    "print(\"Multinomial_Naive_Bayes ({ngram_type}): \")\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84375\n",
      "Precision: 0.8442340791738381\n",
      "Recall: 0.84375\n",
      "F1 Score: 0.8436950490406783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brien\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__C': 1, 'vectorizer__min_df': 0.01} unigrams\n",
    "#{'classifier__C': 1, 'vectorizer__min_df': 0.001} bigrams\n",
    "\n",
    "# Vectoriseer de tekstgegevens\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), min_df= 0.01)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Initialiseer het Logistic Regression model met Lasso penalty\n",
    "model_log1 = LogisticRegression(penalty='l1', solver='saga', C= 1)\n",
    "\n",
    "# Train het model\n",
    "model_log1.fit(X_train, y_train)\n",
    "\n",
    "# Voorspel de labels voor de testset\n",
    "y_pred3 = model_log1.predict(X_test)\n",
    "\n",
    "# evalueer model\n",
    "accuracy = accuracy_score(y_test, y_pred3)\n",
    "precision = precision_score(y_test, y_pred3, average='weighted')  \n",
    "recall = recall_score(y_test, y_pred3, average='weighted')        \n",
    "f1 = f1_score(y_test, y_pred3, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)                                     \n",
    "print(\"Recall:\", recall)                                            \n",
    "print(\"F1 Score:\", f1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.825\n",
      "Precision: 0.825814536340852\n",
      "Recall: 0.825\n",
      "F1 Score: 0.8248905565978737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brien\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__C': 1, 'vectorizer__min_df': 0.01} unigrams\n",
    "#{'classifier__C': 1, 'vectorizer__min_df': 0.001} bigrams\n",
    "\n",
    "# Vectoriseer de tekstgegevens\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df= 0.001)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Initialiseer het Logistic Regression model met Lasso penalty\n",
    "model_log2 = LogisticRegression(penalty='l1', solver='saga', C= 1)\n",
    "\n",
    "# Train het model\n",
    "model_log2.fit(X_train, y_train)\n",
    "\n",
    "# Voorspel de labels voor de testset\n",
    "y_pred4 = model_log2.predict(X_test)\n",
    "\n",
    "# evalueer model\n",
    "accuracy = accuracy_score(y_test, y_pred4)\n",
    "precision = precision_score(y_test, y_pred4, average='weighted')  \n",
    "recall = recall_score(y_test, y_pred4, average='weighted')        \n",
    "f1 = f1_score(y_test, y_pred4, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)                                     \n",
    "print(\"Recall:\", recall)                                            \n",
    "print(\"F1 Score:\", f1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification tree unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65625\n",
      "Precision: 0.6582528881152081\n",
      "Recall: 0.65625\n",
      "F1 Score: 0.6551589012108625\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__ccp_alpha': 0.01, 'classifier__criterion': 'entropy', 'classifier__min_samples_split': 20, 'vectorizer__min_df': 0.05}\n",
    "#{'classifier__ccp_alpha': 0.05, 'classifier__criterion': 'gini', 'classifier__min_samples_split': 2, 'vectorizer__min_df': 0.001}\n",
    "# Vectoriseer de tekstgegevens\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), min_df = 0.05)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])   \n",
    "\n",
    "# Initialiseer het Decision Tree model\n",
    "model_tree1 = DecisionTreeClassifier(criterion= 'entropy', min_samples_split = 20, ccp_alpha= 0.01)\n",
    "\n",
    "# Train het model\n",
    "model_tree1.fit(X_train, y_train)\n",
    "\n",
    "# Voorspel de labels voor de testset\n",
    "y_pred5 = model_tree1.predict(X_test)\n",
    "\n",
    "# evalueer model\n",
    "accuracy = accuracy_score(y_test, y_pred5)\n",
    "precision = precision_score(y_test, y_pred5, average='weighted')  \n",
    "recall = recall_score(y_test, y_pred5, average='weighted')        \n",
    "f1 = f1_score(y_test, y_pred5, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)                                     \n",
    "print(\"Recall:\", recall)                                           \n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification tree bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64375\n",
      "Precision: 0.6437724644475699\n",
      "Recall: 0.64375\n",
      "F1 Score: 0.6437360834407594\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__ccp_alpha': 0.01, 'classifier__criterion': 'entropy', 'classifier__min_samples_split': 20, 'vectorizer__min_df': 0.05}\n",
    "#{'classifier__ccp_alpha': 0.05, 'classifier__criterion': 'gini', 'classifier__min_samples_split': 2, 'vectorizer__min_df': 0.001}\n",
    "# Vectoriseer de tekstgegevens\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), min_df = 0.001)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])   \n",
    "\n",
    "# Initialiseer het Decision Tree model\n",
    "model_tree2 = DecisionTreeClassifier(criterion= 'gini', min_samples_split = 2, ccp_alpha= 0.05)\n",
    "\n",
    "# Train het model\n",
    "model_tree2.fit(X_train, y_train)\n",
    "\n",
    "# Voorspel de labels voor de testset\n",
    "y_pred6 = model_tree2.predict(X_test)\n",
    "\n",
    "# evalueer model\n",
    "accuracy = accuracy_score(y_test, y_pred6)\n",
    "precision = precision_score(y_test, y_pred6, average='weighted')  \n",
    "recall = recall_score(y_test, y_pred6, average='weighted')        \n",
    "f1 = f1_score(y_test, y_pred6, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)                                     \n",
    "print(\"Recall:\", recall)                                           \n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forrest unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85625\n",
      "Precision: 0.8576470588235295\n",
      "Recall: 0.85625\n",
      "F1 Score: 0.8561094819159335\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__criterion': 'entropy', 'classifier__max_features': 'log2', 'classifier__n_estimators': 300, 'vectorizer__min_df': 0.01}\n",
    "#{'classifier__criterion': 'entropy', 'classifier__max_features': 'sqrt', 'classifier__n_estimators': 300, 'vectorizer__min_df': 0.005}\n",
    "\n",
    "# Vectoriseer de tekstgegevens\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), min_df= 0.01)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Initialiseer het Random Forest model\n",
    "model_forrest1 = RandomForestClassifier(oob_score=True,criterion='entropy', max_features= 'log2', n_estimators=300)\n",
    "\n",
    "# Train het model\n",
    "model_forrest1.fit(X_train, y_train)\n",
    "\n",
    "# Voorspel de labels voor de testset\n",
    "y_pred7 = model_forrest1.predict(X_test)\n",
    "\n",
    "# evalueer model\n",
    "accuracy = accuracy_score(y_test, y_pred7)\n",
    "precision = precision_score(y_test, y_pred7, average='weighted')  \n",
    "recall = recall_score(y_test, y_pred7, average='weighted')        \n",
    "f1 = f1_score(y_test, y_pred7, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)                                     \n",
    "print(\"Recall:\", recall)                                            \n",
    "print(\"F1 Score:\", f1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forrest bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8375\n",
      "Precision: 0.8428571428571429\n",
      "Recall: 0.8375\n",
      "F1 Score: 0.836862745098039\n"
     ]
    }
   ],
   "source": [
    "#{'classifier__criterion': 'entropy', 'classifier__max_features': 'log2', 'classifier__n_estimators': 300, 'vectorizer__min_df': 0.01}\n",
    "#{'classifier__criterion': 'entropy', 'classifier__max_features': 'sqrt', 'classifier__n_estimators': 300, 'vectorizer__min_df': 0.005}\n",
    "\n",
    "# Vectoriseer de tekstgegevens\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df= 0.005)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Initialiseer het Random Forest model\n",
    "model_forrest2 = RandomForestClassifier(oob_score=True,criterion='entropy', max_features= 'sqrt', n_estimators=300)\n",
    "\n",
    "# Train het model\n",
    "model_forrest2.fit(X_train, y_train)\n",
    "\n",
    "# Voorspel de labels voor de testset\n",
    "y_pred8 = model_forrest2.predict(X_test)\n",
    "\n",
    "# evalueer model\n",
    "accuracy = accuracy_score(y_test, y_pred8)\n",
    "precision = precision_score(y_test, y_pred8, average='weighted')  \n",
    "recall = recall_score(y_test, y_pred8, average='weighted')        \n",
    "f1 = f1_score(y_test, y_pred8, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)                                     \n",
    "print(\"Recall:\", recall)                                            \n",
    "print(\"F1 Score:\", f1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mc Nemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Perform all model predictions\n",
    "model_predictions = {\n",
    "    'Naive Bayes (unigram)': y_pred1,\n",
    "    'Naive Bayes (bigram)': y_pred2,\n",
    "    'Logistic Regression (unigram)': y_pred3,\n",
    "    'Logistic Regression (bigram)': y_pred4,\n",
    "    'Classification Tree (unigram)': y_pred5,\n",
    "    'Classification Tree (bigram)': y_pred6,\n",
    "    'Random Forest (Unigram)': y_pred7,\n",
    "    'Random Forest (Bigram)': y_pred8,\n",
    "    \n",
    "}\n",
    "\n",
    "def perform_mcnemar_test(y_true, pred1, pred2, model1_name, model2_name):\n",
    "    # Maak de contingentie tabel\n",
    "    table = [[0, 0], [0, 0]]\n",
    "    for i in range(len(y_true)):\n",
    "        if pred1[i] == y_true[i] and pred2[i] == y_true[i]:\n",
    "            table[0][0] += 1\n",
    "        elif pred1[i] == y_true[i] and pred2[i] != y_true[i]:\n",
    "            table[0][1] += 1\n",
    "        elif pred1[i] != y_true[i] and pred2[i] == y_true[i]:\n",
    "            table[1][0] += 1\n",
    "        else:\n",
    "            table[1][1] += 1\n",
    "\n",
    "    # Voer de McNemar test uit\n",
    "    result = mcnemar(table, exact=True)\n",
    "\n",
    "    print(f\"\\nMcNemar's test results for {model1_name} vs {model2_name}\")\n",
    "    print(f\"Statistic: {result.statistic:.4f}\")\n",
    "    print(f\"P-value: {result.pvalue:.4f}\")\n",
    "    print(\"Null hypothesis: the two models have the same error rate\")\n",
    "    print(f\"Reject null hypothesis: {result.pvalue < 0.05}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vergelijking 1 linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "McNemar's test results for Naive Bayes vs Logistic Regression\n",
      "Statistic: 6.0000\n",
      "P-value: 0.0525\n",
      "Null hypothesis: the two models have the same error rate\n",
      "Reject null hypothesis: False\n"
     ]
    }
   ],
   "source": [
    "# 1. Vergelijking van Multinomial Naive Bayes en Logistic Regression\n",
    "perform_mcnemar_test(\n",
    "    y_test,\n",
    "    model_predictions['Naive Bayes (bigram)'],\n",
    "    model_predictions['Logistic Regression (unigram)'],\n",
    "    'Naive Bayes',\n",
    "    'Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is random forrest beter dan linear models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "McNemar's test results for Random Forest vs Naive Bayes\n",
      "Statistic: 6.0000\n",
      "P-value: 0.1153\n",
      "Null hypothesis: the two models have the same error rate\n",
      "Reject null hypothesis: False\n",
      "\n",
      "McNemar's test results for Random Forest vs Logistic Regression\n",
      "Statistic: 11.0000\n",
      "P-value: 0.8388\n",
      "Null hypothesis: the two models have the same error rate\n",
      "Reject null hypothesis: False\n"
     ]
    }
   ],
   "source": [
    "# 2. Vergelijking van Random Forest met lineaire classifiers\n",
    "perform_mcnemar_test(\n",
    "    y_test,\n",
    "    model_predictions['Random Forest (Unigram)'],\n",
    "    model_predictions['Naive Bayes (bigram)'],\n",
    "    'Random Forest',\n",
    "    'Naive Bayes'\n",
    ")\n",
    "\n",
    "perform_mcnemar_test(\n",
    "    y_test,\n",
    "    model_predictions['Random Forest (Unigram)'],\n",
    "    model_predictions['Logistic Regression (unigram)'],\n",
    "    'Random Forest',\n",
    "    'Logistic Regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does bigram improve unigram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "McNemar's test results for Naive Bayes (unigram) vs Naive Bayes (bigram)\n",
      "Statistic: 2.0000\n",
      "P-value: 0.6875\n",
      "Null hypothesis: the two models have the same error rate\n",
      "Reject null hypothesis: False\n",
      "\n",
      "McNemar's test results for Logistic Regression (unigram) vs Logistic Regression (bigram)\n",
      "Statistic: 0.0000\n",
      "P-value: 0.2500\n",
      "Null hypothesis: the two models have the same error rate\n",
      "Reject null hypothesis: False\n",
      "\n",
      "McNemar's test results for Classification Tree (unigram) vs Classification Tree (bigram)\n",
      "Statistic: 18.0000\n",
      "P-value: 0.8714\n",
      "Null hypothesis: the two models have the same error rate\n",
      "Reject null hypothesis: False\n",
      "\n",
      "McNemar's test results for Random Forest (Unigram) vs Random Forest (Bigram)\n",
      "Statistic: 8.0000\n",
      "P-value: 0.6476\n",
      "Null hypothesis: the two models have the same error rate\n",
      "Reject null hypothesis: False\n"
     ]
    }
   ],
   "source": [
    "# 3. Vergelijking van unigrams en bigrams\n",
    "perform_mcnemar_test(\n",
    "    y_test,\n",
    "    model_predictions['Naive Bayes (unigram)'],\n",
    "    model_predictions['Naive Bayes (bigram)'],\n",
    "    'Naive Bayes (unigram)',\n",
    "    'Naive Bayes (bigram)'\n",
    ")\n",
    "\n",
    "perform_mcnemar_test(\n",
    "    y_test,\n",
    "    model_predictions['Logistic Regression (unigram)'],\n",
    "    model_predictions['Logistic Regression (bigram)'],\n",
    "    'Logistic Regression (unigram)',\n",
    "    'Logistic Regression (bigram)'\n",
    ")\n",
    "\n",
    "perform_mcnemar_test(\n",
    "    y_test,\n",
    "    model_predictions['Classification Tree (unigram)'],\n",
    "    model_predictions['Classification Tree (bigram)'],\n",
    "    'Classification Tree (unigram)',\n",
    "    'Classification Tree (bigram)'\n",
    ")\n",
    "\n",
    "perform_mcnemar_test(\n",
    "    y_test,\n",
    "    model_predictions['Random Forest (Unigram)'],\n",
    "    model_predictions['Random Forest (Bigram)'],\n",
    "    'Random Forest (Unigram)',\n",
    "    'Random Forest (Bigram)'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
